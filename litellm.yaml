general_settings:
  store_model_in_db: true
  store_prompts_in_spend_logs: true
  forward_client_headers_to_llm_api: true

litellm_settings:
  proxy_logging: false
  success_logging: true
  drop_params: true

model_list:
  # OpenRouter provider — Anthropic
  # Pricing in LiteLLM DB: claude-sonnet-4.5 ✓, claude-haiku-4.5 ✓
  - model_name: claude-sonnet-4.5
    litellm_params:
      model: openrouter/anthropic/claude-sonnet-4.5
      api_key: os.environ/OPENROUTER_API_KEY

  - model_name: claude-opus-4.6
    litellm_params:
      model: openrouter/anthropic/claude-opus-4.6
      api_key: os.environ/OPENROUTER_API_KEY
    model_info:
      base_model: claude-opus-4-6         # map pricing from LiteLLM DB

  - model_name: claude-haiku-4.5
    litellm_params:
      model: openrouter/anthropic/claude-haiku-4.5
      api_key: os.environ/OPENROUTER_API_KEY

  # OpenRouter provider — OpenAI
  # Pricing in LiteLLM DB: gpt-5-mini ✓, gpt-5.2-codex ✓, gpt-5.2 ✓
  - model_name: text-embedding-3-small
    litellm_params:
      model: openrouter/openai/text-embedding-3-small
      api_key: os.environ/OPENROUTER_API_KEY
      drop_params: true
      additional_drop_params: [ "stop" ]
    model_info:
      base_model: text-embedding-3-small  # map pricing from LiteLLM DB

  - model_name: gpt-5-mini
    litellm_params:
      model: openrouter/openai/gpt-5-mini
      api_key: os.environ/OPENROUTER_API_KEY
      drop_params: true
      additional_drop_params: [ "stop" ]

  - model_name: gpt-5.2-codex
    litellm_params:
      model: openrouter/openai/gpt-5.2-codex
      api_key: os.environ/OPENROUTER_API_KEY
      drop_params: true
      additional_drop_params: [ "stop" ]

  - model_name: gpt-5.1-codex-mini
    litellm_params:
      model: openrouter/openai/gpt-5.1-codex-mini
      api_key: os.environ/OPENROUTER_API_KEY
      drop_params: true
      additional_drop_params: [ "stop" ]
    model_info:
      base_model: gpt-5.1-codex-mini     # map pricing from LiteLLM DB

  - model_name: gpt-5.2
    litellm_params:
      model: openrouter/openai/gpt-5.2
      api_key: os.environ/OPENROUTER_API_KEY
      drop_params: true
      additional_drop_params: [ "stop" ]

  # OpenRouter provider — Google
  # Pricing in LiteLLM DB: gemini-3-flash-preview ✓
  - model_name: gemini-3-flash-preview
    litellm_params:
      model: openrouter/google/gemini-3-flash-preview
      api_key: os.environ/OPENROUTER_API_KEY
      drop_params: true

  # OpenRouter provider — Z.ai
  - model_name: glm-5
    litellm_params:
      model: openrouter/z-ai/glm-5
      api_key: os.environ/OPENROUTER_API_KEY
      drop_params: true
    model_info:
      input_cost_per_token: 0.0000003     # $0.30/M — not in LiteLLM DB
      output_cost_per_token: 0.00000255    # $2.55/M

  # OpenRouter provider — DeepSeek
  # Pricing in LiteLLM DB: deepseek-v3.2 ✓
  - model_name: deepseek-v3.2
    litellm_params:
      model: openrouter/deepseek/deepseek-v3.2
      api_key: os.environ/OPENROUTER_API_KEY
      drop_params: true

  # OpenRouter provider — MiniMax
  - model_name: openrouter/minimax-m2.5
    litellm_params:
      model: openrouter/minimax/minimax-m2.5
      api_key: os.environ/OPENROUTER_API_KEY
      drop_params: true
    model_info:
      base_model: minimax/MiniMax-M2.5    # map pricing from LiteLLM DB

  # Ollama local models
  - model_name: qwen3-coder:30b
    litellm_params:
      model: openai/qwen3-coder:30b
      api_base: http://host.docker.internal:11434/v1/
      api_key: ollama
    model_info:
      supports_function_calling: true
      supports_parallel_function_calling: true
      supports_reasoning: false
      supports_response_schema: true
      input_cost_per_token: 0
      output_cost_per_token: 0

  - model_name: qwen3-coder-next
    litellm_params:
      model: openai/qwen3-coder-next
      api_base: http://host.docker.internal:11434/v1/
      api_key: ollama
    model_info:
      supports_function_calling: true
      supports_parallel_function_calling: true
      supports_reasoning: false
      supports_response_schema: true
      input_cost_per_token: 0
      output_cost_per_token: 0

  - model_name: glm-4.7-flash
    litellm_params:
      model: openai/glm-4.7-flash
      api_base: http://host.docker.internal:11434/v1/
      api_key: ollama
    model_info:
      supports_function_calling: true
      supports_parallel_function_calling: true
      supports_reasoning: true
      supports_response_schema: true
      input_cost_per_token: 0
      output_cost_per_token: 0

  - model_name: qwen3-embedding:4b
    litellm_params:
      model: openai/qwen3-embedding:4b
      api_base: http://host.docker.internal:11434/v1/
      api_key: ollama
      drop_params: true
    model_info:
      supports_function_calling: false
      supports_parallel_function_calling: false
      supports_reasoning: false
      supports_response_schema: true
      input_cost_per_token: 0
      output_cost_per_token: 0

  - model_name: qwen2.5-coder:7b
    litellm_params:
      model: openai/qwen2.5-coder:7b
      api_base: http://host.docker.internal:11434/v1/
      api_key: ollama
      drop_params: true
    model_info:
      supports_function_calling: false
      supports_parallel_function_calling: false
      supports_reasoning: false
      supports_response_schema: true
      input_cost_per_token: 0
      output_cost_per_token: 0

  - model_name: phi3.5:3.8b-mini-instruct-q4_K_M
    litellm_params:
      model: openai/phi3.5:3.8b-mini-instruct-q4_K_M
      api_base: http://host.docker.internal:11434/v1/
      api_key: ollama
      drop_params: true
    model_info:
      supports_function_calling: false
      supports_parallel_function_calling: false
      supports_reasoning: false
      supports_response_schema: true
      input_cost_per_token: 0
      output_cost_per_token: 0
