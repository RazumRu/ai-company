# LLM provider keys (passed to LiteLLM via docker-compose)
OPENAI_API_KEY=
GEMINI_API_KEY=

# Tool keys
GITHUB_PAT_TOKEN=
TAVILY_API_KEY=

# Set to true to use local Ollama models instead of cloud providers
# LLM_USE_OFFLINE_MODEL=false
